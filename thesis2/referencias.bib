@article{moreno2018revisiones,
  title={Revisiones Sistem{\'a}ticas: definici{\'o}n y nociones b{\'a}sicas},
  author={Moreno, Bego{\~n}a and Mu{\~n}oz, Maximiliano and Cuellar, Javier and Domancic, Stefan and Villanueva, Julio},
  journal={Revista cl{\'\i}nica de periodoncia, implantolog{\'\i}a y rehabilitaci{\'o}n oral},
  volume={11},
  number={3},
  pages={184--186},
  year={2018},
  publisher={Sociedad de Periodoncia de Chile. Sociedad de Implantolog{\'\i}a Oral de Chile~…}
}

@book{zobel,
    author    = "Zobel J",
    title     = "Writing for Computer Science",
    year      = "2015",
    publisher = "Springer",
    ?_edition  = "Third Edition"
}

@book{swales,
    author    = "Swales J. M. and Feak C. B.",
    title     = "Academic Writing for Graduate Students",
    year      = "2012",
    publisher = "The university of Michigan Press",
    ?_edition  = "Third Edition"
}


@inproceedings{10.1145/3419635.3419643,
author = {Lan, Xiaojuan and Bai, Juyang and Li, Meng and Li, Jiajun},
title = {Fish Image Classification Using Deep Convolutional Neural Network},
year = {2020},
isbn = {9781450387729},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3419635.3419643},
doi = {10.1145/3419635.3419643},
abstract = {Scientific research on species composition and geographical distribution of marine
organisms is of great significance to the research of marine resources and the protection
of rare species of marine life. In these studies, divers or underwater robots are
often used to collect biological images, which are then manually classified by relevant
experts. Manual-based classification is not only time-consuming but also prone to
misjudge. Deep learning algorithms have also been applied in this field, but the classification
performance is poor in general, mainly due to the low image quality and the small
number of collected images. In response to this challenging, a fish classification
algorithm based on Inception-V3 is proposed in this paper. First, data augmentation
is realized by scaling, inverting, and panning of original images. Then transfer learning
method is applied to improve the prediction accuracy. Experimental results show that
the proposed method can effectively improve the classification accuracy, reaching
about 89\% for fish species.},
booktitle = {Proceedings of the 2020 International Conference on Computers, Information Processing and Advanced Education},
pages = {18–22},
numpages = {5},
keywords = {transfer learning, Inception-V3, fish classification},
location = {Ottawa, ON, Canada},
series = {CIPAE 2020}
}

@inproceedings{10.1145/3325917.3325934,
author = {Manandhar, Nibha and Burris, John W.},
title = {An Application of Image Classification to Saltwater Fish Identification in Louisiana Fisheries},
year = {2019},
isbn = {9781450366359},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3325917.3325934},
doi = {10.1145/3325917.3325934},
abstract = {Fish identification is a challenge to recreational anglers, but critically important
to the management of fisheries. The state of Louisiana currently provides printed
illustrations of species as the sole aid to anglers for the process of fish identification.
This work describes the application of Google's TensorFlow machine learning library
to the task of fish identification as a case study on the application of the image
classification capabilities. We describe the implementation and results of the project.},
booktitle = {Proceedings of the 2019 3rd International Conference on Information System and Data Mining},
pages = {129–132},
numpages = {4},
keywords = {Fish identification, Image classification, Machine learning},
location = {Houston, TX, USA},
series = {ICISDM 2019}
}

@inproceedings{20.500.12724/11174,
author = {Mejía, Rodrigo and Rosales, Gianfranco},
title = {Sistema de detección y clasificación de peces utilizando visión computacional},
year = {2020},
publisher = {Actas del II Congreso Internacional de Ingeniería de Sistemas },
address = {Lima, LIM, Peru},
url = {https://hdl.handle.net/20.500.12724/11174},
doi = {20.500.12724/11174},
abstract = {La gestión de los recursos hidrobiológicos implica tanto el aspecto ecológico a través del equilibrio del ecosistema, como el aspecto económico mediante el control de la cantidad y calidad de los recursos pesqueros producidos en el Perú. En la actualidad, labores relacionadas a esta gestión son realizadas por empresas privadas y entidades del Estado como el Imarpe. La misión de estas es proteger la calidad de los recursos que llegan a los hogares de millones de peruanos. Esta investigación busca desarrollar un sistema para la detección, clasificación y, finalmente, la medición de diversas especies de peces, utilizando técnicas de visión computacional como el algoritmo SURF y redes neuronales convolucionales. Las pruebas, utilizando dos especies de peces, demostraron que la identificación alcanza un nivel de precisión del 90 \% y que la clasificación alcanza una precisión del 80 \%. Estos valores se obtienen bajo determinadas condiciones que se comentan en el desarrollo del artículo.},
booktitle = {Innovando la educación en tecnología},
pages = {127-141},
numpages = {14},
keywords = {Visión por ordenador, Redes neuronales artificiales	, Peces, Artificial neural networks	},
location = {Lima,LIM,Perú}
}

@article{WildFish ,
   abstract = {Fish recognition is an important task to understand the marine ecosystem and biodiversity. It is often challenging to identify fish species in the wild, due to the following difficulties. First, most fish benchmarks are small-scale, which may limit the representation power of machine learning models. Second, the number of fish species is huge, and there may still exist unknown categories in our planet. The traditional classifiers often fail to deal with this open-set scenario. Third, certain fish species are highly-confused. It is often hard to figure out the subtle differences, only by the unconstrained images. Motivated by these facts, we introduce a large-scale Wild-Fish benchmark for fish recognition in the wild. Specifically, we make three contributions in this paper. First, WildFish is the largest image data set for wild fish recognition, to our best knowledge. It consists of 1000 fish categories with 54,459 unconstrained images, allowing to train high-capacity models for automatic fish classification. Second, we propose a novel open-set fish classification task for realistic scenarios, and investigate the open-set deep learning framework with a number of practical designs. Third, we propose a novel fine-grained recognition task, with the guidance of pairwise textual descriptions. Via leveraging the comparison knowledge in the sentence, we design a multi-modal fish net to effectively distinguish two confused categories in a pair. Finally, we release WildFish (https://github.com/PeiqinZhuang/WildFish), in order to bring benefit to more research studies in multimedia and beyond.},
   author = {Peiqin Zhuang and Yali Wang and Yu Qiao},
   city = {New York, NY, USA},
   doi = {10.1145/3240508},
   journal = {Proceedings of the 26th ACM international conference on Multimedia},
   keywords = {Deep Learning,Fine-Grained Recognition,Fish Classification,Open-Set Classification,Vision-Text Modeling},
   month = {10},
   publisher = {ACM},
   title = {WildFish: A Large Benchmark for Fish Recognition in the Wild},
   url = {https://doi.org/10.1145/3240508.3240616},
   year = {2018},
}

@INPROCEEDINGS{8371919,
  author={Chen, Guang and Sun, Peng and Shang, Yi},
  booktitle={2017 IEEE 29th International Conference on Tools with Artificial Intelligence (ICTAI)}, 
  title={Automatic Fish Classification System Using Deep Learning}, 
  year={2017},
  volume={},
  number={},
  pages={24-29},
  doi={10.1109/ICTAI.2017.00016}}

@article{Cui2020,
   abstract = {Recently, human being's curiosity has been expanded from the land to the sky and the sea. Besides sending people to explore the ocean and outer space, robots are designed for some tasks dangerous for living creatures. Take the ocean exploration for an example. There are many projects or competitions on the design of Autonomous Underwater Vehicle (AUV) which attracted many interests. Authors of this article have learned the necessity of platform upgrade from a previous AUV design project, and would like to share the experience of one task extension in the area of fish detection. Because most of the embedded systems have been improved by fast growing computing and sensing technologies, which makes them possible to incorporate more and more complicated algorithms. In an AUV, after acquiring surrounding information from sensors, how to perceive and analyse corresponding information for better judgement is one of the challenges. The processing procedure can mimic human being's learning routines. An advanced system with more computing power can facilitate deep learning feature, which exploit many neural network algorithms to simulate human brains. In this paper, a convolutional neural network (CNN) based fish detection method was proposed. The training data set was collected from the Gulf of Mexico by a digital camera. To fit into this unique need, three optimization approaches were applied to the CNN: data augmentation, network simplification, and training process speed up. Data augmentation transformation provided more learning samples; the network was simplified to accommodate the artificial neural network; the training process speed up is introduced to make the training process more time efficient. Experimental results showed that the proposed model is promising, and has the potential to be extended to other underwear objects.},
   author = {Suxia Cui and Yu Zhou and Yonghui Wang and Lujun Zhai},
   doi = {10.1155/2020/3738108},
   journal = {Applied Computational Intelligence and Soft Computing},
   publisher = {Hindawi Limited},
   title = {Fish Detection Using Deep Learning},
   volume = {2020},
   url = {https://dl.acm.org/doi/pdf/10.1145/3419635.3419643},
   year = {2020},
}
@article{ContinousLearning ,
   abstract = {Our research focuses on a new data flow architecture in neural network training called Continuous Neural Network Learning (CNNL) whose main objective is the reduction of data required to train a neural network. In real-world applications, much of the raw data used in deep learning algorithms do not have a large labeled datasets readily available for training. CNNL seeks to allow for more efficient neural network implementations by significantly reducing the necessary size of the labeled dataset and secondarily decreasing the processing and training time required to achieve reasonable accuracy. Not only is a CNNL system shown to be able to achieve impressive results with little tuning on standardized datasets, but the initialization is as low as 150 images. While this research only the first step and requires further refinement for real world application, it proves the potential for a CNNL system.},
   author = {Michael Baucum and Eric Savage and Daniel Belotto and Prannoy Mupparaju and Sayre Jeannet and Carlos W Morato},
   city = {New York, New York, USA},
   doi = {10.1145/3094243},
   journal = {Proceedings of the 2017 International Conference on Deep Learning Technologies  - ICDLT '17},
   keywords = {CCS Concepts Computing methodologies → Machine learning → Machine learning approaches → Neural networks Keywords Neural Network,CNNL,Continuous,Convolutional Neural Network,Deep Learning,Learning,Semi Supervised,Transfer Learning},
   month = {6},
   publisher = {ACM Press},
   title = {Semi-supervised Deep Continuous Learning},
   url = {http://dx.doi.org/10.1145/3094243.3094247},
   year = {2017},
}
@INPROCEEDINGS{ImageNet,
  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Kai Li and Li Fei-Fei},
  booktitle={2009 IEEE Conference on Computer Vision and Pattern Recognition}, 
  title={ImageNet: A large-scale hierarchical image database}, 
  year={2009},
  volume={},
  number={},
  pages={248-255},
  abstract={The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called “ImageNet”, a large-scale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 500-1000 clean and full resolution images. This will result in tens of millions of annotated images organized by the semantic hierarchy of WordNet. This paper offers a detailed analysis of ImageNet in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show that ImageNet is much larger in scale and diversity and much more accurate than the current image datasets. Constructing such a large-scale database is a challenging task. We describe the data collection scheme with Amazon Mechanical Turk. Lastly, we illustrate the usefulness of ImageNet through three simple applications in object recognition, image classification and automatic object clustering. We hope that the scale, accuracy, diversity and hierarchical structure of ImageNet can offer unparalleled opportunities to researchers in the computer vision community and beyond.},
  keywords={},
  doi={10.1109/CVPR.2009.5206848},
  ISSN={1063-6919},
  month={June},}
  
@article{LeCun1989,
   abstract = {The ability of learning networks to generalize can be greatly enhanced by providing constraints from the task domain. This paper demonstrates how such constraints can be integrated into a backpropagation network through the architecture of the network. This approach has been successfully applied to the recognition of handwritten zip code digits provided by the U.S. Postal Service. A single network learns the entire recognition operation, going from the normalized image of the character to the final classification.},
   author = {Y. LeCun and B. Boser and J. S. Denker and D. Henderson and R. E. Howard and W. Hubbard and L. D. Jackel},
   doi = {10.1162/NECO.1989.1.4.541},
   issn = {0899-7667},
   issue = {4},
   journal = {Neural Computation},
   month = {12},
   pages = {541-551},
   publisher = {MIT Press - Journals},
   title = {Backpropagation Applied to Handwritten Zip Code Recognition},
   volume = {1},
   url = {https://ieeexplore.ieee.org/document/6795724},
   year = {1989},
}
@article{LeCun1990,
   abstract = {We present an application of back-propagation networks to handwritten digit recognition. Minimal preprocessing of the data was required, but architecture of the network was highly constrained and specifically designed for the task. The input of the network consists of normalized images of isolated digits. The method has 1 \% error rate and about a 9\% reject rate on zipcode digits provided by the U.S. Postal Service.},
   author = {Le Cun and Jackel Henderson and Y Le Cun and J S Denker and D Henderson and R E Howard and W Hubbard and L D Jackel},
   journal = {Advances in neural information processing systems 2},
   pages = {396-404},
   title = {Handwritten Digit Recognition with a Back-Propagation Network},
   url = {https://papers.nips.cc/paper/1989/file/53c3bce66e43be4f209556518c2fcb54-Paper.pdf},
   year = {1990},
}

@article{Yanmei2020,
   abstract = {This paper proposes an improved convolutional neural network structure which greatly reduces the scale of network training parameters. It uses the global average pooling algorithm instead of the full connection algorithm to improve the LeNet-5 network. The number of convolution kernels is increased, while the number of subsampling layers is reduced to an optimal value. After verification with MINST handwritten Arabic numeral data set, the results show that the improved network training parameters are only 34.8\% of the original, and the recognition accuracy can achieve 99.3\%.},
   author = {He Yanmei and Wang Bo and Zhu Zhaomin},
   city = {New York, NY, USA},
   doi = {10.1145/3443467},
   isbn = {9781450387811},
   journal = {Proceedings of the 2020 4th International Conference on Electronic Information Technology and Computer Engineering},
   keywords = {CCS CONCEPTS  Computing methodologies KEYWORDS convolutional neural network,LeNet-5,digits recognition},
   publisher = {ACM},
   title = {An improved LeNet-5 model for Image Recognition},
   url = {https://doi.org/10.1145/3443467.3443797},
   year = {2020},
}
@thesis{Lopez2010,
   author = {Patricia Lopez},
   city = {Cantabria},
   institution = {Universidad de Cantabria},
   title = {Desarrollo de sistemas de tiempo real basado en componentes utilizando modelos de comportamiento reactivos},
   url = {https://www.tesisenred.net/bitstream/handle/10803/10639/TesisPLM.pdf?sequence=1},
   year = {2010},
}

@article{Yani2019,
   abstract = {Nails are one part of the fingers and toes, by observing the shape and the condition of the nails, health expert can find out information about a person's health. However, this sometimes not realized and ignored by society, even though many diseases that can be seen through the condition of the nails and the shape of the nails are one of the systemic diseases. This research was conducted to detect abnormalities in the nail based on digital images. The detected abnormalities are terry's nails in the hand which can represent systemic diseases, while the method used is the Convolutional Neural Network (CNN) method. This research uses Tensorflow Inception-V3 architecture model with the transfer learning method where the results of the experiments that have been done are obtained with 95.24\% accuracy.},
   author = {Muhamad Yani and Budhi Irawan and Casi Setiningsih},
   doi = {10.1088/1742-6596/1201/1/012052},
   issn = {17426596},
   issue = {1},
   journal = {Journal of Physics: Conference Series},
   keywords = {Convolutional Neural Network,Nail,Terrys nail},
   month = {5},
   publisher = {Institute of Physics Publishing},
   title = {Application of Transfer Learning Using Convolutional Neural Network Method for Early Detection of Terry's Nail},
   volume = {1201},
   url = {https://www.researchgate.net/publication/333593451_Application_of_Transfer_Learning_Using_Convolutional_Neural_Network_Method_for_Early_Detection_of_Terry's_Nail},
   year = {2019},
}

@misc{Simonyan2015,
   abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3 × 3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisa-tion and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
   author = {Karen Simonyan and Andrew Zisserman},
   keywords = {()},
   title = {VERY DEEP CONVOLUTIONAL NETWORKS FOR LARGE-SCALE IMAGE RECOGNITION},
   url = {http://www.robots.ox.ac.uk/},
   year = {2015},
}

@misc{Szegedy2014,
   abstract = {We propose a deep convolutional neural network architecture codenamed Inception , which was responsible for setting the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC14). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. This was achieved by a carefully crafted design that allows for increasing the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for ILSVRC14 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.},
   author = {Christian Szegedy and Wei Liu and Pierre Sermanet and Scott Reed and Dragomir Anguelov and Dumitru Erhan and Vincent Vanhoucke and Andrew Rabinovich},
   title = {Going deeper with convolutions},
   year = {2014},
}

@misc{He2015,
      title={Deep Residual Learning for Image Recognition}, 
      author={Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},
      year={2015},
      eprint={1512.03385},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{Tan2020,
   abstract = {Convolutional Neural Networks (ConvNets) are commonly developed at a fixed resource budget, and then scaled up for better accuracy if more resources are available. In this paper, we systematically study model scaling and identify that carefully balancing network depth, width, and resolution can lead to better performance. Based on this observation, we propose a new scaling method that uniformly scales all dimensions of depth/width/resolution using a simple yet highly effective compound coefficient. We demonstrate the effectiveness of this method on scaling up MobileNets and ResNet. To go even further, we use neural architecture search to design a new baseline network and scale it up to obtain a family of models, called EfficientNets, which achieve much better accuracy and efficiency than previous ConvNets. In particular, our EfficientNet-B7 achieves state-of-the-art 84.3\% top-1 accuracy on ImageNet, while being 8.4x smaller and 6.1x faster on inference than the best existing ConvNet. Our EfficientNets also transfer well and achieve state-of-the-art accuracy on CIFAR-100 (91.7\%), Flowers (98.8\%), and 3 other transfer learning datasets, with an order of magnitude fewer parameters. Source code is at https: //github.com/tensorflow/tpu/tree/ master/models/official/efficientnet.},
   author = {Mingxing Tan and Quoc V Le},
   title = {EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks},
   url = {https://arxiv.org/pdf/1905.11946.pdf},
   year = {2020},
}




//////////////////////////////////////////////
@MISC{neuronas,
    author = {{Magiquo}},
    title = {AtomicRedes neuronales o imitar al cerebro humano?},
    year = {2019},
    note = {[Online; accessed December 05, 2021]},
    url = {https://magiquo.com/wp-content/uploads/2019/11/neurona.png}
}

@MISC{MLP,
    author = {{CodigoFuente}},
    title = {Redes neuronales profundas – Tipos y Características},
    year = {2019},
    note = {[Online; accessed December 05, 2021]},
    url = {https://www.codigofuente.org/redes-neuronales-profundas-tipos-caracteristicas/}
}

@MISC{convoluciones,
    author = {{Diego Calvo}},
    title = {red-neuronal-convolucional},
    year = {2019},
    note = {[Online; accessed December 05, 2021]},
    url = {https://www.diegocalvo.es/red-neuronal-convolucional/}
}


@MISC{CNN-Arquitectura,
    author = {{Diego Calvo}},
    title = {red-neuronal-convolucional-arquitectura},
    year = {2019},
    note = {[Online; accessed December 05, 2021]},
    url = {https://www.diegocalvo.es/red-neuronal-convolucional/red-neuronal-convolucional-arquitectura/}
}

@MISC{LE-NET5,
    author = {{Viet Tra
Jaeyoung Kim
Sheraz Ali Khan
Jongmyon Kim}},
    title = {red-neuronal-convolucional-arquitectura},
    year = {2019},
    note = {[Online; accessed December 05, 2021]},
    url = {https://www.researchgate.net/figure/The-LeNet-5-Architecture-a-convolutional-neural-network_fig4_321586653}
}
@MISC{pooling,
    author = {{Muhamad Yani,
    Budhi Irawan,
    Casi Setianingsih}},
    title = {Application of Transfer Learning Using Convolutional Neural Network Method for Early Detection of Terry’s Nail},
    year = {2019},
    note = {[Online; accessed December 05, 2021]},
    url = {https://www.researchgate.net/figure/Illustration-of-Max-Pooling-and-Average-Pooling-Figure-2-above-shows-an-example-of-max_fig2_333593451}
}

@MISC{transfer-learning,
    author = {{Wenjin Taoa,
    Md Al-Aminb,
    Haodong Chena,
    Ming C. Leua,
    Zhaozheng Yinc,
    Ruwen Qinb}},
    title = {Real-Time Assembly Operation Recognition with Fog Computing and Transfer Learning for Human-Centered Intelligent Manufacturing},
    year = {2020},
    note = {[Online; accessed December 05, 2021]},
    url = {https://www.researchgate.net/figure/The-architecture-of-our-transfer-learning-model_fig4_342400905}
}



@MISC{modelos,
    author = {{IchiPro}},
    title = {4 modelos de CNN previamente entrenados para usar en visión artificial con aprendizaje por transferencia},
    year = {2020},
    note = {[Online; accessed December 05, 2021]},
    url = {https://ichi.pro/es/4-modelos-de-cnn-previamente-entrenados-para-usar-en-vision-artificial-con-aprendizaje-por-transferencia-9370731228668}
}

@MISC{EfficientNetB0,
    author = {Tashin Ahmed, Noor Hossain Sabab},
    title = {Classification and understanding of cloud structures via satellite images with EfficientUNet},
    year = {2020},
    note = {[Online; accessed December 05, 2021]},
    url = {https://www.researchgate.net/figure/Architecture-of-EfficientNet-B0-with-MBConv-as-Basic-building-blocks_fig4_344410350}
}

@MISC{DataModelos ,
   author = {Orhan Yalcin},
   journal = {Pre-Trained Model Performances},
   title = {Pre-Trained Model Performances.csv},
   url = {https://gist.github.com/ogyalcin/052f2df49b3288e62086aa0e5fd25fcd},
   year = {2020},
}

@article{Redmon2015,
archivePrefix = {arXiv},
arxivId = {1506.02640},
author = {Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
doi = {10.48550/arxiv.1506.02640},
eprint = {1506.02640},
file = {:C\:/Users/CESAR/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Redmon et al. - 2015 - You Only Look Once Unified, Real-Time Object Detection.pdf:pdf},
isbn = {9781467388504},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
month = {jun},
pages = {779--788},
publisher = {IEEE Computer Society},
title = {{You Only Look Once: Unified, Real-Time Object Detection}},
url = {https://arxiv.org/abs/1506.02640v5},
volume = {2016-December},
year = {2015}
}
@article{Alsmadi2022,
author = {Alsmadi, Mutasem K. and Almarashdeh, Ibrahim},
doi = {10.1016/J.JKSUCI.2020.07.005},
file = {:C\:/Users/CESAR/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Alsmadi, Almarashdeh - 2022 - A survey on fish classification techniques.pdf:pdf},
issn = {1319-1578},
journal = {Journal of King Saud University - Computer and Information Sciences},
keywords = {Color features,Features extraction,Fish classification algorithms,Image segmentation,Shape features,Texture features},
month = {may},
number = {5},
pages = {1625--1638},
publisher = {Elsevier},
title = {{A survey on fish classification techniques}},
volume = {34},
year = {2022}
}
@article{Overfitting,
author = {Calvo,Ismael },
doi = {67976/1},
journal = {Journal of King Saud University - Computer and Information Sciences},
keywords = {Color features,Features extraction,Fish classification algorithms,Image segmentation,Shape features,Texture features},
month = {may},
number = {5},
pages = {1625--1638},
publisher = {Elsevier},
title = {{A survey on fish classification techniques}},
volume = {34},
year = {2022}
}
@phdthesis{67976/1 ,
author = {Calvo,Ismael},
title = {Algoritmos de aprendizaje automático para detección de fraudes con tarjeta de crédito: Análisis y comparativa },
publisher = {Universidad politécnica de madrid},
address = {Madrid, España},
year = {2021}
}

@misc{yolov5,
  author       = {Glenn Jocher and
                  Ayush Chaurasia and
                  Alex Stoken and
                  Jirka Borovec and
                  NanoCode012 and
                  Yonghye Kwon and
                  Kalen Michael and
                  TaoXie and
                  Jiacong Fang and
                  imyhxy and
                  Lorna and
                  曾逸夫(Zeng Yifu) and
                  Colin Wong and
                  Abhiram V and
                  Diego Montes and
                  Zhiqiang Wang and
                  Cristi Fati and
                  Jebastin Nadar and
                  Laughing and
                  UnglvKitDe and
                  Victor Sonck and
                  tkianai and
                  yxNONG and
                  Piotr Skalski and
                  Adam Hogan and
                  Dhruv Nair and
                  Max Strobel and
                  Mrinal Jain},
  title        = {{ultralytics/yolov5: v7.0 - YOLOv5 SOTA Realtime 
                   Instance Segmentation}},
  month        = nov,
  year         = 2022,
  publisher    = {Zenodo},
  version      = {v7.0},
  doi          = {10.5281/zenodo.7347926},
  url          = {https://doi.org/10.5281/zenodo.7347926}
}

@misc{unidet,
author = {Zhou, Xingyi; Koltun, Vladlen and Krahenbuhl, Philipp },
doi = {2102.13086},
year = {2022},
month = {Apr},
title = {{Simple Multi-dataset Detection}},
url = {https://arxiv.org/pdf/2102.13086.pdf},
}
@article{cascadercnn,
abstract = {In object detection, an intersection over union (IoU) threshold is required
to define positives and negatives. An object detector, trained with low IoU
threshold, e.g. 0.5, usually produces noisy detections. However, detection
performance tends to degrade with increasing the IoU thresholds. Two main
factors are responsible for this: 1) overfitting during training, due to
exponentially vanishing positive samples, and 2) inference-time mismatch
between the IoUs for which the detector is optimal and those of the input
hypotheses. A multi-stage object detection architecture, the Cascade R-CNN, is
proposed to address these problems. It consists of a sequence of detectors
trained with increasing IoU thresholds, to be sequentially more selective
against close false positives. The detectors are trained stage by stage,
leveraging the observation that the output of a detector is a good distribution
for training the next higher quality detector. The resampling of progressively
improved hypotheses guarantees that all detectors have a positive set of
examples of equivalent size, reducing the overfitting problem. The same cascade
procedure is applied at inference, enabling a closer match between the
hypotheses and the detector quality of each stage. A simple implementation of
the Cascade R-CNN is shown to surpass all single-model object detectors on the
challenging COCO dataset. Experiments also show that the Cascade R-CNN is
widely applicable across detector architectures, achieving consistent gains
independently of the baseline detector strength. The code will be made
available at https://github.com/zhaoweicai/cascade-rcnn.},
archivePrefix = {arXiv},
arxivId = {1712.00726},
author = {Cai, Zhaowei and Vasconcelos, Nuno},
doi = {10.1109/CVPR.2018.00644},
eprint = {1712.00726},
file = {:C\:/Users/CESAR/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Cai, Vasconcelos - 2017 - Cascade R-CNN Delving into High Quality Object Detection(3).pdf:pdf},
isbn = {9781538664209},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
month = {dec},
pages = {6154--6162},
publisher = {IEEE Computer Society},
title = {{Cascade R-CNN: Delving into High Quality Object Detection}},
url = {https://arxiv.org/abs/1712.00726v1},
year = {2017}
}
