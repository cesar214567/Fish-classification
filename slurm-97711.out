2022-10-19 23:47:16.837224: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-10-19 23:47:16.981144: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2022-10-19 23:47:17.028322: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2022-10-19 23:47:18.396352: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/cesar.madera/.local/lib/python3.9/site-packages/cv2/../../lib64:/usr/local/cuda-11.4/lib64:/opt/apps/Python-3.9.2/lib:/usr/local/cuda-11.4/nvvm/lib64:/usr/local/cuda-11.4/extras/CUPTI/lib64:/usr/local/cuda-11.4/lib64:/opt/apps/gcc-9.2.0/lib64:/opt/apps/gcc-9.2.0/lib
2022-10-19 23:47:18.396443: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/cesar.madera/.local/lib/python3.9/site-packages/cv2/../../lib64:/usr/local/cuda-11.4/lib64:/opt/apps/Python-3.9.2/lib:/usr/local/cuda-11.4/nvvm/lib64:/usr/local/cuda-11.4/extras/CUPTI/lib64:/usr/local/cuda-11.4/lib64:/opt/apps/gcc-9.2.0/lib64:/opt/apps/gcc-9.2.0/lib
2022-10-19 23:47:18.396455: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
10.2
True
device is:  cuda
Keras version: 2.10.0
Read train images
Load folder BET (Index: 0)
./NatureConservancy/train/BET/*.jpg
Load folder DOL (Index: 1)
./NatureConservancy/train/DOL/*.jpg
Load folder LAG (Index: 2)
./NatureConservancy/train/LAG/*.jpg
Load folder NoF (Index: 3)
./NatureConservancy/train/NoF/*.jpg
Load folder OTHER (Index: 4)
./NatureConservancy/train/OTHER/*.jpg
Load folder SHARK (Index: 5)
./NatureConservancy/train/SHARK/*.jpg
Load folder YFT (Index: 6)
./NatureConservancy/train/YFT/*.jpg
Read train data time: 27.46 seconds
Convert to numpy...
Reshape...
Convert to float...
Train shape: (2058, 3, 244, 244)
2058 train samples
Total memory: 15843721216
Free memory: 15840575488
Used memory: 3145728
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1         [-1, 64, 224, 224]           1,792
              ReLU-2         [-1, 64, 224, 224]               0
            Conv2d-3         [-1, 64, 224, 224]          36,928
              ReLU-4         [-1, 64, 224, 224]               0
         MaxPool2d-5         [-1, 64, 112, 112]               0
            Conv2d-6        [-1, 128, 112, 112]          73,856
              ReLU-7        [-1, 128, 112, 112]               0
            Conv2d-8        [-1, 128, 112, 112]         147,584
              ReLU-9        [-1, 128, 112, 112]               0
        MaxPool2d-10          [-1, 128, 56, 56]               0
           Conv2d-11          [-1, 256, 56, 56]         295,168
             ReLU-12          [-1, 256, 56, 56]               0
           Conv2d-13          [-1, 256, 56, 56]         590,080
             ReLU-14          [-1, 256, 56, 56]               0
           Conv2d-15          [-1, 256, 56, 56]         590,080
             ReLU-16          [-1, 256, 56, 56]               0
           Conv2d-17          [-1, 256, 56, 56]         590,080
             ReLU-18          [-1, 256, 56, 56]               0
        MaxPool2d-19          [-1, 256, 28, 28]               0
           Conv2d-20          [-1, 512, 28, 28]       1,180,160
             ReLU-21          [-1, 512, 28, 28]               0
           Conv2d-22          [-1, 512, 28, 28]       2,359,808
             ReLU-23          [-1, 512, 28, 28]               0
           Conv2d-24          [-1, 512, 28, 28]       2,359,808
             ReLU-25          [-1, 512, 28, 28]               0
           Conv2d-26          [-1, 512, 28, 28]       2,359,808
             ReLU-27          [-1, 512, 28, 28]               0
        MaxPool2d-28          [-1, 512, 14, 14]               0
           Conv2d-29          [-1, 512, 14, 14]       2,359,808
             ReLU-30          [-1, 512, 14, 14]               0
           Conv2d-31          [-1, 512, 14, 14]       2,359,808
             ReLU-32          [-1, 512, 14, 14]               0
           Conv2d-33          [-1, 512, 14, 14]       2,359,808
             ReLU-34          [-1, 512, 14, 14]               0
           Conv2d-35          [-1, 512, 14, 14]       2,359,808
             ReLU-36          [-1, 512, 14, 14]               0
        MaxPool2d-37            [-1, 512, 7, 7]               0
AdaptiveAvgPool2d-38            [-1, 512, 7, 7]               0
           Linear-39                 [-1, 4096]     102,764,544
      BatchNorm1d-40                 [-1, 4096]           8,192
             ReLU-41                 [-1, 4096]               0
          Dropout-42                 [-1, 4096]               0
           Linear-43                 [-1, 1024]       4,195,328
             ReLU-44                 [-1, 1024]               0
          Dropout-45                 [-1, 1024]               0
           Linear-46                    [-1, 8]           8,200
          Softmax-47                    [-1, 8]               0
================================================================
Total params: 127,000,648
Trainable params: 106,976,264
Non-trainable params: 20,024,384
----------------------------------------------------------------
Input size (MB): 0.57
Forward/backward pass size (MB): 238.64
Params size (MB): 484.47
Estimated Total Size (MB): 723.68
----------------------------------------------------------------
Total memory: 15843721216
Free memory: 14281342976
Used memory: 1562378240
epoch:  1
tensor(2.0797, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.8450, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.8384, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7704, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.8739, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.8253, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.8438, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.8452, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.8136, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.8202, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7486, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7735, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7540, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.6772, device='cuda:0', grad_fn=<NllLossBackward0>)
[[ 0  0  0 11  5  0 13]
 [ 0  0  0  6  3  0  5]
 [ 0  0  0  6  3  0  2]
 [ 0  0  0 40  7  0 18]
 [ 0  0  0 14 10  0 13]
 [ 0  0  0 11  6  0 14]
 [ 0  0  0 42 10  0 55]]
aciertos:  105
fallas:  294
loss_tensor(1.9102)_folds_7
epoch:  2
tensor(1.7228, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7433, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.6803, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7083, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7553, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.8032, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7249, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7347, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7011, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7434, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.6701, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.6630, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7509, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.8055, device='cuda:0', grad_fn=<NllLossBackward0>)
[[ 0  0  0 12  1  0 16]
 [ 0  0  0  9  0  0  5]
 [ 0  0  0  5  1  0  5]
 [ 0  0  0 37  2  0 26]
 [ 0  0  0 12  3  0 22]
 [ 0  0  0 10  0  0 21]
 [ 0  0  0 42  2  0 63]]
aciertos:  103
fallas:  294
loss_tensor(1.9102)_folds_7
epoch:  3
tensor(1.7569, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7520, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7507, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7346, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.6927, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.6891, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7303, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.6734, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7214, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.8174, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.6612, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7221, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.6683, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.6682, device='cuda:0', grad_fn=<NllLossBackward0>)
[[ 0  0  0  5  8  0 16]
 [ 0  0  0  3  5  0  6]
 [ 0  0  0  0  6  0  5]
 [ 0  0  0 20  5  0 40]
 [ 0  0  0  1 17  0 19]
 [ 0  0  0  2  7  0 22]
 [ 0  0  0  6 25  0 76]]
aciertos:  113
fallas:  294
loss_tensor(1.8859)_folds_7
epoch:  4
tensor(1.6937, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.6500, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.6549, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.6731, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7113, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.6692, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.6891, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.6977, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.6872, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.6979, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.6610, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.6240, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.6481, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7597, device='cuda:0', grad_fn=<NllLossBackward0>)
[[ 0  0  0  9  4  0 16]
 [ 0  0  0  7  1  0  6]
 [ 0  0  0  0  4  0  7]
 [ 0  0  0 27  3  0 35]
 [ 0  0  0  7  9  0 21]
 [ 0  0  0  3  3  0 25]
 [ 0  0  0 14  4  0 89]]
aciertos:  125
fallas:  294
loss_tensor(1.8512)_folds_7
epoch:  5
slurmstepd: error: *** JOB 97711 ON g001 CANCELLED AT 2022-10-19T23:49:59 ***
