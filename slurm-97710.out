2022-10-19 23:42:13.612204: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-10-19 23:42:13.756226: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2022-10-19 23:42:13.803030: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2022-10-19 23:42:14.709841: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/cesar.madera/.local/lib/python3.9/site-packages/cv2/../../lib64:/usr/local/cuda-11.4/lib64:/opt/apps/Python-3.9.2/lib:/usr/local/cuda-11.4/nvvm/lib64:/usr/local/cuda-11.4/extras/CUPTI/lib64:/usr/local/cuda-11.4/lib64:/opt/apps/gcc-9.2.0/lib64:/opt/apps/gcc-9.2.0/lib
2022-10-19 23:42:14.709935: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/cesar.madera/.local/lib/python3.9/site-packages/cv2/../../lib64:/usr/local/cuda-11.4/lib64:/opt/apps/Python-3.9.2/lib:/usr/local/cuda-11.4/nvvm/lib64:/usr/local/cuda-11.4/extras/CUPTI/lib64:/usr/local/cuda-11.4/lib64:/opt/apps/gcc-9.2.0/lib64:/opt/apps/gcc-9.2.0/lib
2022-10-19 23:42:14.709947: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
10.2
True
device is:  cuda
Keras version: 2.10.0
Read train images
Load folder ALB (Index: 0)
./NatureConservancy/train/ALB/*.jpg
Load folder BET (Index: 1)
./NatureConservancy/train/BET/*.jpg
Load folder DOL (Index: 2)
./NatureConservancy/train/DOL/*.jpg
Load folder LAG (Index: 3)
./NatureConservancy/train/LAG/*.jpg
Load folder NoF (Index: 4)
./NatureConservancy/train/NoF/*.jpg
Load folder OTHER (Index: 5)
./NatureConservancy/train/OTHER/*.jpg
Load folder SHARK (Index: 6)
./NatureConservancy/train/SHARK/*.jpg
Load folder YFT (Index: 7)
./NatureConservancy/train/YFT/*.jpg
Read train data time: 48.74 seconds
Convert to numpy...
Reshape...
Convert to float...
Train shape: (3777, 3, 244, 244)
3777 train samples
Total memory: 15843721216
Free memory: 15840575488
Used memory: 3145728
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1         [-1, 64, 224, 224]           1,792
              ReLU-2         [-1, 64, 224, 224]               0
            Conv2d-3         [-1, 64, 224, 224]          36,928
              ReLU-4         [-1, 64, 224, 224]               0
         MaxPool2d-5         [-1, 64, 112, 112]               0
            Conv2d-6        [-1, 128, 112, 112]          73,856
              ReLU-7        [-1, 128, 112, 112]               0
            Conv2d-8        [-1, 128, 112, 112]         147,584
              ReLU-9        [-1, 128, 112, 112]               0
        MaxPool2d-10          [-1, 128, 56, 56]               0
           Conv2d-11          [-1, 256, 56, 56]         295,168
             ReLU-12          [-1, 256, 56, 56]               0
           Conv2d-13          [-1, 256, 56, 56]         590,080
             ReLU-14          [-1, 256, 56, 56]               0
           Conv2d-15          [-1, 256, 56, 56]         590,080
             ReLU-16          [-1, 256, 56, 56]               0
           Conv2d-17          [-1, 256, 56, 56]         590,080
             ReLU-18          [-1, 256, 56, 56]               0
        MaxPool2d-19          [-1, 256, 28, 28]               0
           Conv2d-20          [-1, 512, 28, 28]       1,180,160
             ReLU-21          [-1, 512, 28, 28]               0
           Conv2d-22          [-1, 512, 28, 28]       2,359,808
             ReLU-23          [-1, 512, 28, 28]               0
           Conv2d-24          [-1, 512, 28, 28]       2,359,808
             ReLU-25          [-1, 512, 28, 28]               0
           Conv2d-26          [-1, 512, 28, 28]       2,359,808
             ReLU-27          [-1, 512, 28, 28]               0
        MaxPool2d-28          [-1, 512, 14, 14]               0
           Conv2d-29          [-1, 512, 14, 14]       2,359,808
             ReLU-30          [-1, 512, 14, 14]               0
           Conv2d-31          [-1, 512, 14, 14]       2,359,808
             ReLU-32          [-1, 512, 14, 14]               0
           Conv2d-33          [-1, 512, 14, 14]       2,359,808
             ReLU-34          [-1, 512, 14, 14]               0
           Conv2d-35          [-1, 512, 14, 14]       2,359,808
             ReLU-36          [-1, 512, 14, 14]               0
        MaxPool2d-37            [-1, 512, 7, 7]               0
AdaptiveAvgPool2d-38            [-1, 512, 7, 7]               0
           Linear-39                 [-1, 4096]     102,764,544
      BatchNorm1d-40                 [-1, 4096]           8,192
             ReLU-41                 [-1, 4096]               0
          Dropout-42                 [-1, 4096]               0
           Linear-43                 [-1, 1024]       4,195,328
             ReLU-44                 [-1, 1024]               0
          Dropout-45                 [-1, 1024]               0
           Linear-46                    [-1, 8]           8,200
          Softmax-47                    [-1, 8]               0
================================================================
Total params: 127,000,648
Trainable params: 106,976,264
Non-trainable params: 20,024,384
----------------------------------------------------------------
Input size (MB): 0.57
Forward/backward pass size (MB): 238.64
Params size (MB): 484.47
Estimated Total Size (MB): 723.68
----------------------------------------------------------------
Total memory: 15843721216
Free memory: 14281342976
Used memory: 1562378240
epoch:  1
tensor(2.0783, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7662, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7771, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7707, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7414, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7399, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7865, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.8269, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7606, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.8371, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7710, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7403, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7426, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7803, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.6933, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.8038, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7753, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.8106, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7827, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7549, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.8155, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.8139, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.8518, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7535, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7722, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7344, device='cuda:0', grad_fn=<NllLossBackward0>)
[[241   0   0   0   0   0   0   0]
 [ 25   0   0   0   0   0   0   1]
 [ 16   0   0   0   0   0   0   4]
 [ 11   0   0   0   0   0   0   0]
 [ 62   0   0   0   0   0   0   0]
 [ 49   0   0   0   0   0   0   0]
 [ 23   0   0   0   0   0   0   0]
 [ 91   0   0   0   0   0   0  17]]
aciertos:  258
fallas:  540
loss_tensor(1.7980)_folds_7
epoch:  2
tensor(1.8207, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7851, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7722, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7681, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7662, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7733, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.6724, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7587, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7815, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.6829, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7659, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7760, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7326, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7081, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7334, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7337, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7335, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7456, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.8388, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7849, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7824, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.8273, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.8413, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7487, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.8361, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.8208, device='cuda:0', grad_fn=<NllLossBackward0>)
[[225   0   0   0   0   0   0  16]
 [ 20   0   0   0   0   0   0   6]
 [ 15   0   0   0   0   0   0   5]
 [ 11   0   0   0   0   0   0   0]
 [ 52   0   0   0   0   0   0  10]
 [ 44   0   0   0   0   0   0   5]
 [ 20   0   0   0   0   0   0   3]
 [ 82   0   0   0   0   0   0  26]]
aciertos:  251
fallas:  540
loss_tensor(1.7980)_folds_7
epoch:  3
tensor(1.6532, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.8163, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7182, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7815, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7235, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7271, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7887, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7579, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7733, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.8125, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.8053, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7190, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7604, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7293, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7820, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7720, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7767, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.8119, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7540, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7891, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7520, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7438, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.8129, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.8112, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7637, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.8280, device='cuda:0', grad_fn=<NllLossBackward0>)
[[220   0   0   0   0   0   0  21]
 [ 20   0   0   0   0   0   0   6]
 [ 15   0   0   0   0   0   0   5]
 [ 10   0   0   0   0   0   0   1]
 [ 51   0   0   0   0   0   0  11]
 [ 44   0   0   0   0   0   0   5]
 [ 20   0   0   0   0   0   0   3]
 [ 83   0   0   0   0   0   0  25]]
aciertos:  245
fallas:  540
loss_tensor(1.7980)_folds_7
epoch:  4
tensor(1.8048, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7646, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7496, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.8405, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7120, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7918, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.8126, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7792, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7815, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.6877, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7387, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7905, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7272, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7269, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7752, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.6709, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7658, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7364, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7456, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.6816, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.6751, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.6972, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7778, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7260, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7345, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.6861, device='cuda:0', grad_fn=<NllLossBackward0>)
[[213   0   0   0   0   0   0  28]
 [ 21   0   0   0   0   0   0   5]
 [ 13   0   0   0   0   0   0   7]
 [ 11   0   0   0   0   0   0   0]
 [ 51   0   0   0   0   0   0  11]
 [ 46   0   0   0   0   0   0   3]
 [ 20   0   0   0   0   0   0   3]
 [ 80   0   0   0   0   0   0  28]]
aciertos:  241
fallas:  540
loss_tensor(1.7980)_folds_7
epoch:  5
tensor(1.7321, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7907, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.6909, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7940, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7235, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7097, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7545, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7587, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.6892, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7420, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.6253, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.6392, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.8650, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.6999, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.8201, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7376, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7218, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7819, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7320, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.6768, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7804, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7334, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7322, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7165, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7267, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.6246, device='cuda:0', grad_fn=<NllLossBackward0>)
slurmstepd: error: *** JOB 97710 ON g001 CANCELLED AT 2022-10-19T23:47:13 ***
