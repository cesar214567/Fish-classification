2022-10-19 23:35:06.425820: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-10-19 23:35:06.569835: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2022-10-19 23:35:06.616580: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2022-10-19 23:35:07.523375: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/cesar.madera/.local/lib/python3.9/site-packages/cv2/../../lib64:/usr/local/cuda-11.4/lib64:/opt/apps/Python-3.9.2/lib:/usr/local/cuda-11.4/nvvm/lib64:/usr/local/cuda-11.4/extras/CUPTI/lib64:/usr/local/cuda-11.4/lib64:/opt/apps/gcc-9.2.0/lib64:/opt/apps/gcc-9.2.0/lib
2022-10-19 23:35:07.523464: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/cesar.madera/.local/lib/python3.9/site-packages/cv2/../../lib64:/usr/local/cuda-11.4/lib64:/opt/apps/Python-3.9.2/lib:/usr/local/cuda-11.4/nvvm/lib64:/usr/local/cuda-11.4/extras/CUPTI/lib64:/usr/local/cuda-11.4/lib64:/opt/apps/gcc-9.2.0/lib64:/opt/apps/gcc-9.2.0/lib
2022-10-19 23:35:07.523476: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
10.2
True
device is:  cuda
Keras version: 2.10.0
Read train images
Load folder ALB (Index: 0)
./NatureConservancy/train/ALB/*.jpg
Load folder BET (Index: 1)
./NatureConservancy/train/BET/*.jpg
Load folder DOL (Index: 2)
./NatureConservancy/train/DOL/*.jpg
Load folder LAG (Index: 3)
./NatureConservancy/train/LAG/*.jpg
Load folder NoF (Index: 4)
./NatureConservancy/train/NoF/*.jpg
Load folder OTHER (Index: 5)
./NatureConservancy/train/OTHER/*.jpg
Load folder SHARK (Index: 6)
./NatureConservancy/train/SHARK/*.jpg
Load folder YFT (Index: 7)
./NatureConservancy/train/YFT/*.jpg
Read train data time: 48.0 seconds
Convert to numpy...
Reshape...
Convert to float...
Train shape: (3777, 3, 244, 244)
3777 train samples
Total memory: 15843721216
Free memory: 15840575488
Used memory: 3145728
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1         [-1, 64, 224, 224]           1,792
              ReLU-2         [-1, 64, 224, 224]               0
            Conv2d-3         [-1, 64, 224, 224]          36,928
              ReLU-4         [-1, 64, 224, 224]               0
         MaxPool2d-5         [-1, 64, 112, 112]               0
            Conv2d-6        [-1, 128, 112, 112]          73,856
              ReLU-7        [-1, 128, 112, 112]               0
            Conv2d-8        [-1, 128, 112, 112]         147,584
              ReLU-9        [-1, 128, 112, 112]               0
        MaxPool2d-10          [-1, 128, 56, 56]               0
           Conv2d-11          [-1, 256, 56, 56]         295,168
             ReLU-12          [-1, 256, 56, 56]               0
           Conv2d-13          [-1, 256, 56, 56]         590,080
             ReLU-14          [-1, 256, 56, 56]               0
           Conv2d-15          [-1, 256, 56, 56]         590,080
             ReLU-16          [-1, 256, 56, 56]               0
           Conv2d-17          [-1, 256, 56, 56]         590,080
             ReLU-18          [-1, 256, 56, 56]               0
        MaxPool2d-19          [-1, 256, 28, 28]               0
           Conv2d-20          [-1, 512, 28, 28]       1,180,160
             ReLU-21          [-1, 512, 28, 28]               0
           Conv2d-22          [-1, 512, 28, 28]       2,359,808
             ReLU-23          [-1, 512, 28, 28]               0
           Conv2d-24          [-1, 512, 28, 28]       2,359,808
             ReLU-25          [-1, 512, 28, 28]               0
           Conv2d-26          [-1, 512, 28, 28]       2,359,808
             ReLU-27          [-1, 512, 28, 28]               0
        MaxPool2d-28          [-1, 512, 14, 14]               0
           Conv2d-29          [-1, 512, 14, 14]       2,359,808
             ReLU-30          [-1, 512, 14, 14]               0
           Conv2d-31          [-1, 512, 14, 14]       2,359,808
             ReLU-32          [-1, 512, 14, 14]               0
           Conv2d-33          [-1, 512, 14, 14]       2,359,808
             ReLU-34          [-1, 512, 14, 14]               0
           Conv2d-35          [-1, 512, 14, 14]       2,359,808
             ReLU-36          [-1, 512, 14, 14]               0
        MaxPool2d-37            [-1, 512, 7, 7]               0
AdaptiveAvgPool2d-38            [-1, 512, 7, 7]               0
           Linear-39                 [-1, 4096]     102,764,544
      BatchNorm1d-40                 [-1, 4096]           8,192
             ReLU-41                 [-1, 4096]               0
          Dropout-42                 [-1, 4096]               0
           Linear-43                 [-1, 1024]       4,195,328
             ReLU-44                 [-1, 1024]               0
          Dropout-45                 [-1, 1024]               0
           Linear-46                    [-1, 8]           8,200
          Softmax-47                    [-1, 8]               0
================================================================
Total params: 127,000,648
Trainable params: 106,976,264
Non-trainable params: 20,024,384
----------------------------------------------------------------
Input size (MB): 0.57
Forward/backward pass size (MB): 238.64
Params size (MB): 484.47
Estimated Total Size (MB): 723.68
----------------------------------------------------------------
Total memory: 15843721216
Free memory: 14281342976
Used memory: 1562378240
epoch:  1
tensor(2.0851, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7615, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7920, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7772, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7745, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7936, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7985, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.8361, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7991, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.8365, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7650, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7887, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7657, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7356, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7743, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7710, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7621, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7796, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7631, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7517, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7613, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7298, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.8153, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7742, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7193, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.8953, device='cuda:0', grad_fn=<NllLossBackward0>)
[[241   0   0   0   0   0   0   0]
 [ 24   0   0   0   0   0   0   2]
 [ 16   0   0   0   0   0   0   4]
 [ 11   0   0   0   0   0   0   0]
 [ 62   0   0   0   0   0   0   0]
 [ 48   0   0   0   0   0   0   1]
 [ 23   0   0   0   0   0   0   0]
 [ 97   0   0   0   0   0   0  11]]
aciertos:  252
fallas:  540
loss_tensor(1.8068)_folds_7
epoch:  2
tensor(1.7815, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7701, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7818, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7774, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7897, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7662, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7043, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7833, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7584, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7615, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7975, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.8085, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7348, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.8154, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7348, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7656, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7657, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.8050, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7337, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.8459, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7741, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7043, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7584, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7738, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7371, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.9268, device='cuda:0', grad_fn=<NllLossBackward0>)
[[239   0   0   0   0   0   0   2]
 [ 24   0   0   0   0   0   0   2]
 [ 17   0   0   0   0   0   0   3]
 [ 11   0   0   0   0   0   0   0]
 [ 62   0   0   0   0   0   0   0]
 [ 47   0   0   0   0   0   0   2]
 [ 23   0   0   0   0   0   0   0]
 [ 92   0   0   0   0   0   0  16]]
aciertos:  255
fallas:  540
loss_tensor(1.8019)_folds_7
epoch:  3
tensor(1.7103, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7780, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7817, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7740, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7349, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7861, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7195, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7347, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7449, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7559, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.8004, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.8210, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.8453, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7527, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7721, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7799, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.8124, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7815, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7107, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7204, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.8291, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7703, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7506, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7476, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.8071, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7605, device='cuda:0', grad_fn=<NllLossBackward0>)
[[241   0   0   0   0   0   0   0]
 [ 24   0   0   0   0   0   0   2]
 [ 19   0   0   0   0   0   0   1]
 [ 11   0   0   0   0   0   0   0]
 [ 62   0   0   0   0   0   0   0]
 [ 49   0   0   0   0   0   0   0]
 [ 23   0   0   0   0   0   0   0]
 [ 90   0   0   0   0   0   0  18]]
aciertos:  259
fallas:  540
loss_tensor(1.7954)_folds_7
epoch:  4
tensor(1.7572, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7506, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7274, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7375, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7974, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7498, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7047, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7508, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7818, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7490, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.8181, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.6881, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7896, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7740, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.8374, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7886, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7503, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7522, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7584, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.6880, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.6886, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.8130, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.8135, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.8130, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7429, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(2.0037, device='cuda:0', grad_fn=<NllLossBackward0>)
[[239   0   0   0   0   0   0   2]
 [ 24   0   0   0   0   0   0   2]
 [ 17   0   0   0   0   0   0   3]
 [ 11   0   0   0   0   0   0   0]
 [ 62   0   0   0   0   0   0   0]
 [ 49   0   0   0   0   0   0   0]
 [ 22   0   0   0   0   0   0   1]
 [ 87   0   0   0   0   0   0  21]]
aciertos:  260
fallas:  540
loss_tensor(1.7931)_folds_7
epoch:  5
tensor(1.7594, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7823, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7272, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7977, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7584, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7663, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.6878, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7852, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.6959, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7665, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7740, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7506, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7748, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7190, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7117, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7896, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7586, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7506, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7659, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.8365, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.8049, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7193, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7268, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.8131, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7813, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7875, device='cuda:0', grad_fn=<NllLossBackward0>)
[[238   0   0   0   0   0   0   3]
 [ 23   0   0   0   0   0   0   3]
 [ 15   0   0   0   0   0   0   5]
 [ 11   0   0   0   0   0   0   0]
 [ 62   0   0   0   0   0   0   0]
 [ 47   0   0   0   0   0   0   2]
 [ 22   0   0   0   0   0   0   1]
 [ 83   0   0   0   0   0   0  25]]
aciertos:  263
fallas:  540
loss_tensor(1.7877)_folds_7
epoch:  6
tensor(1.7642, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.8053, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7037, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7302, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7954, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.8286, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.6724, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7662, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7115, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7271, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7662, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7818, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7662, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7271, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7584, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.8053, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7823, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7583, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7037, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7924, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.8365, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7349, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7662, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7977, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.6656, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.8686, device='cuda:0', grad_fn=<NllLossBackward0>)
[[238   0   0   0   0   0   0   3]
 [ 24   0   0   0   0   0   0   2]
 [ 15   0   0   0   0   0   0   5]
 [ 11   0   0   0   0   0   0   0]
 [ 62   0   0   0   0   0   0   0]
 [ 49   0   0   0   0   0   0   0]
 [ 23   0   0   0   0   0   0   0]
 [ 88   0   0   0   0   0   0  20]]
aciertos:  258
fallas:  540
loss_tensor(1.7877)_folds_7
epoch:  7
tensor(1.7740, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.9146, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7428, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.6965, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7818, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7428, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7584, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7662, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7272, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7268, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7541, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7272, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7273, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7783, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7038, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7080, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.8232, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7548, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7446, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.8521, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7626, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7579, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7759, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7891, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.7882, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(1.8407, device='cuda:0', grad_fn=<NllLossBackward0>)
[[188   0   0   0   0   0   0  53]
 [ 18   0   0   0   0   0   0   8]
 [ 16   0   0   0   0   0   0   4]
 [ 11   0   0   0   0   0   0   0]
 [ 43   0   0   0   0   0   0  19]
 [ 40   0   0   0   0   0   0   9]
 [  7   0   0   0   0   0   0  16]
 [ 67   0   0   0   0   0   0  41]]
aciertos:  229
fallas:  540
loss_tensor(1.7877)_folds_7
epoch:  8
slurmstepd: error: *** JOB 97708 ON g001 CANCELLED AT 2022-10-19T23:42:09 ***
